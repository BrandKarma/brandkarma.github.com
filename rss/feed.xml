<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>MnO2's Blog</title>
    <link href="http://blog.mno2.org/rss/feed.xml" rel="self" />
    <link href="http://blog.mno2.org" />
    <id>http://blog.mno2.org/rss/feed.xml</id>
    <author>
        <name>Paul Meng</name>
        <email>mno2@mno2.org</email>
    </author>
    <updated>2015-02-27T00:00:00Z</updated>
    <entry>
    <title>My view toward Apache Kafka</title>
    <link href="http://blog.mno2.org/posts/2015-02-27-my-view-toward-apache-kafka.html" />
    <id>http://blog.mno2.org/posts/2015-02-27-my-view-toward-apache-kafka.html</id>
    <published>2015-02-27T00:00:00Z</published>
    <updated>2015-02-27T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p><a href="http://kafka.apache.org/">Apache Kafka</a> is a distributed, fault tolerent high throughput transaction log system. Most of the marketing materials coin it as a message broker or publish-subscribe system. I couldn’t say those claims are wrong because in most of the architectures it does serve as a centralized logging system, and pass the message between different parts of the architectural diagram. However, I like to say it as a transaction log system because its API is relatively low-level in semantic comparing to RabbitMQ and others systems that would first come up in your mind when mentioned for the (task) message queue. The offset of the consumers are managed by the consuemr themselves (though starting from 0.8, the offsets of simple consumers are stored in Zookeeper for simple use cases.) This distiction might be because just like <a href="https://medium.com/@jeeyoungk/why-i-love-databases-1d4cc433685f">any data store abstraction are leaky</a>, the message queue system when put in different scenario, are also leaky. And Apache Kafka lies on the end of low-level API with high throuhput end of the spectrum.</p>
<p>Apache Kafka is carefully well-designed and there have been several articles from its creator and Confluent, the company backing the Apache Kafka.</p>
<ul>
<li><a href="http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying">The Log: What every software engineer should know about real-time data’s unifying abstraction</a></li>
<li><a href="http://blog.confluent.io/2015/02/25/stream-data-platform-1/">PUTTING APACHE KAFKA TO USE 1</a>, <a href="http://blog.confluent.io/2015/02/25/stream-data-platform-2/">2</a></li>
<li><a href="http://blog.confluent.io/2015/01/29/making-sense-of-stream-processing/">STREAM PROCESSING, EVENT SOURCING, REACTIVE, CEP… AND MAKING SENSE OF IT ALL</a></li>
</ul>
<p><a href="https://medium.com/@jeeyoungk/how-sharding-works-b4dec46b3f6">The intrinsic difficulty of distributed systems</a> also applied to Kafka. As <a href="https://aphyr.com/posts/293-call-me-maybe-kafka">Kyle Kingsbury</a> pointed out. The claims that Kafka picks CA out of CAP theorem doesn’t make much sense, because the undefinedness during partition would eventually hurts the availability. Kafka is more like a CP system under certain constraints. And the singular point failure mode was indicated in the article.</p>
<p>From my experience with Kafka, I would say it is quite stable to use, as long as you deploy it with the correct settings and architecture. And you python binding is good enough for minor stream processing cases. You don’t have to go for complex stream processing framework such as Apache Samza and Twitter’s Storm. If you are familiar with Haskell’s lazy evaluation. Any of the Kafka’s topic is just like a variable binding with a lazy stream behind. The variable is bound to a stream. And what you have to do is just use a pure function to transform the stream into another binding. I could denote them in <code>let topic-2 = f . g $ topic-1</code>. And with a few of sinking streams consuming the data into database or search engine’s index. This programming model ease the pain of data store replication, since the “thing” behind the variable are not in the same memory address space, and the cost of copy is huge.</p>
<p>Kafka saves a lot of its states (consumer group offset, replica leader, partition infos) and configuration in Zookeeper. Therefore you must make sure Zookeeper doesn’t go wrong. And you could infer from that most of the operational task is to set a variable in Zookeeper. Like to reset an offset of a given consumer group and partition, it is setting a variable in Zookeeper directory. And to let it consume from start, you just delete that variable remember the offset of that consumer group.</p>
<p>The following is my notes to common operational tasks.</p>
<h4 id="create-topic">create topic</h4>
<pre><code>./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 4  --topic hello-world</code></pre>
<h4 id="tail-messages-in-a-topic">tail messages in a topic</h4>
<pre><code>bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic hello-world</code></pre>
<h4 id="cat-messages-in-a-topic">cat messages in a topic</h4>
<pre><code>bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic hello-world --from-beginning</code></pre>
<h4 id="write-to-a-topic">write to a topic</h4>
<pre><code>bin/kafka-console-producer.sh --zookeeper localhost:2181 --topic hello-world</code></pre>
<h4 id="get-the-offset">get the offset</h4>
<pre><code>bin/kafka-run-class.sh kafka.tools.GetOffsetShell --topic=hello-world --server=kafka://127.0.0.1:8001  --time=-1</code></pre>
<h4 id="dump-segment-files">dump segment files</h4>
<pre><code>bin/kafka-run-class.sh kafka.tools.DumpLogSegments kafka-logs/hello-world-0/00000000000034305339.kafka</code></pre>
<h4 id="get-the-offset-before-a-certain-time">get the offset before a certain time</h4>
<pre><code>bin/kafka-run-class.sh kafka.tools.GetOffsetShell --topic=hello-world --server=kafka://127.0.0.1:8001  --time=1406877452000 --offsets=1</code></pre>
<h4 id="print-message-greater-than-an-offset">print message greater than an offset</h4>
<pre><code>bin/kafka-run-class.sh kafka.tools.SimpleConsumerShell --server=kafka://127.0.0.1:8001 --topic=hello-world --offset=34305339 --fetchsize=5000 --print-message</code></pre>
]]></summary>
</entry>
<entry>
    <title>PostGIS with SQLAlchemy</title>
    <link href="http://blog.mno2.org/posts/2015-01-06-postgis-with-sqlalchemy.html" />
    <id>http://blog.mno2.org/posts/2015-01-06-postgis-with-sqlalchemy.html</id>
    <published>2015-01-06T00:00:00Z</published>
    <updated>2015-01-06T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Postgres is often said as the open-source world Oracle DB. It is shipped with different kind of index type. The default is B-tree, the others are R-Tree and GIST and GIN respectively. <a href="http://postgis.net/docs/manual-2.1/">PostGIS</a> is built for indexing geometry objects, and it is built on Postgres’s GIST type index. On AWS RDS you could also switch it on by following the <a href="http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.PostgreSQL.CommonDBATasks.html#Appendix.PostgreSQL.CommonDBATasks.PostGIS">instructions</a></p>
<p>To take the most of it and build fast prototypes, we still need ORM abstraction for PostGIS extension. The popular Sqlalchemy ORM in python world comes with geoalchemy extension. It provides functions from PostGIS.</p>
<p>To use it, you have to import it into your model file</p>
<pre><code>from geoalchemy2 import Geometry</code></pre>
<p>And define the column with <code>Geometry</code> class</p>
<pre><code>geom = Column(Geometry(‘POINT’), nullable=False)</code></pre>
<p>And insert to the model by assigning it a <code>WKTElement</code> with correct srid</p>
<pre><code>s = ‘POINT(%s %s)’ %(geocode[“latitude”], geocode[“longitude”])
l.geom = WKTElement(s, srid=4326)</code></pre>
<p>Then you could do the common operation using <a href="http://geoalchemy-2.readthedocs.org/en/latest/spatial_functions.html">spatial functions</a></p>
<pre><code>from geoalchemy2 import functions</code></pre>
]]></summary>
</entry>

</feed>
